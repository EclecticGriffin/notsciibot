{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyAgent:\n",
    "    def __init__(self, num_acts, num_features, num_units=10, learning_rate=0.01, decay=0.95):\n",
    "        self.n_actions = num_acts\n",
    "        self.n_features = num_features\n",
    "        self.learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        \n",
    "        self.ep_obs, self.ep_acts, self.ep_rew = [], [], []\n",
    "        \n",
    "        self._build(num_units)\n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "        \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def _build(self, num_units):\n",
    "        # Input Info\n",
    "        self.obs = tf.placeholder(tf.float32, (None, self.n_features))\n",
    "        self.acts = tf.placeholder(tf.int32, (None,))\n",
    "        self.rew = tf.placeholder(tf.float32, (None,))\n",
    "        \n",
    "        # Model Layers\n",
    "        h1 = tf.layers.dense(self.obs, num_units, activation=tf.nn.tanh, \n",
    "                             kernel_initializer=tf.random_normal_initializer())\n",
    "        h2 = tf.layers.dense(h1, num_units, activation=tf.nn.tanh, \n",
    "                             kernel_initializer=tf.random_normal_initializer())\n",
    "        out = tf.layers.dense(h2, self.n_actions, activation=None, \n",
    "                             kernel_initializer=tf.random_normal_initializer())\n",
    "        \n",
    "        self.probabilities = tf.nn.softmax(out)\n",
    "      #  self.action_chooser = tf.multinomial(probabilities,1)\n",
    "        \n",
    "        neg_log_prob = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.probabilities, labels=self.acts)\n",
    "        loss = tf.reduce_mean(neg_log_prob * self.rew)\n",
    "        \n",
    "        self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(loss)\n",
    "    \n",
    "    def choose_action(self,obs):\n",
    "        prob_weights = self.sess.run(self.probabilities, feed_dict={self.obs:np.array(obs).reshape(-1,self.n_features)})\n",
    "        action = np.random.choice(range(prob_weights.shape[1]), p=prob_weights.ravel())\n",
    "        return action\n",
    "    \n",
    "    def store(self, obs, action, reward):\n",
    "        self.ep_obs.append(obs)\n",
    "        self.ep_acts.append(action)\n",
    "        self.ep_rew.append(reward)\n",
    "        \n",
    "    def process_rewards(self):\n",
    "        discounted_rewards = np.zeros_like(self.ep_rew)\n",
    "        run_sum = 0\n",
    "        for i in reversed(range(len(self.ep_rew))):\n",
    "            run_sum *= self.decay\n",
    "            run_sum += self.ep_rew[i]\n",
    "            discounted_rewards[i] = run_sum\n",
    "        \n",
    "        discounted_rewards -= np.mean(discounted_rewards)\n",
    "        if np.std(discounted_rewards) != 0:\n",
    "            discounted_rewards /= np.std(discounted_rewards)\n",
    "        return discounted_rewards\n",
    "    \n",
    "    def train(self):\n",
    "        discounted_rewards = self.process_rewards()\n",
    "        \n",
    "        self.sess.run(self.train_op,feed_dict={self.obs: np.array(self.ep_obs).reshape(-1,self.n_features),\n",
    "                                          self.acts: np.array(self.ep_acts),\n",
    "                                          self.rew: np.array(discounted_rewards)})\n",
    "        \n",
    "        self.ep_obs, self.ep_acts, self.ep_rew = [], [], []\n",
    "        \n",
    "        return discounted_rewards    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "agent = PolicyAgent(num_acts=env.action_space.n,num_features=env.observation_space.shape[0],\n",
    "                    num_units=30,learning_rate=0.0001, decay=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./cart_models/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "saver.restore(agent.sess,'./cart_models/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = saver.save(agent.sess, './cart_models/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_episodes = 1000000000000\n",
    "#max_steps = 100000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "471.084, 463.221, 472.047, 479.964, 478.497, 473.127, 475.979, 473.948, 459.202, 474.59, 480.84, 473.335, 472.37, 481.456, 471.217, 476.657, 473.651, 468.482, 479.783, 477.35, 470.839, 470.046, 476.89, 478.128, 475.543, 475.005, 476.844, 480.17, 475.758, 464.278, 472.1, 471.247, 458.989, 467.127, 469.829, 466.555, 471.182, 474.007, 463.562, 474.723, 474.725, 478.3, 476.114, 474.222, 468.155, 465.88, 468.904, 474.245, 476.65, 478.465, 465.469, 468.902, 473.315, 468.092, 469.177, 463.575, 474.976, 475.831, 468.76, 468.734, 471.056, 475.028, 469.857, 469.509, 468.93, 467.985, 468.784, 471.849, 462.678, 465.925, 465.072, 462.802, 467.008, 468.273, 468.598, 467.77, 460.865, 465.588, 467.497, 471.979, 474.678, 464.586, 473.796, 459.602, 460.055, 469.174, 468.553, 469.489, 472.462, 473.702, 462.946, 474.98, 481.765, 480.228, 479.385, 477.874, 477.398, 464.908, 466.715, 471.328, 477.469, 473.031, 476.471, 474.126, 472.917, 480.76, 475.531, 471.402, 464.657, 462.896, 466.984, 467.595, 471.535, 464.231, 472.47, 467.105, 472.623, 462.077, 472.782, "
     ]
    }
   ],
   "source": [
    "avg = []\n",
    "for episode_num in range(max_episodes):\n",
    "    obs_prev = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "#         env.render()\n",
    "        action = agent.choose_action(obs_prev)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        \n",
    "        agent.store(obs_prev, action, reward)\n",
    "        \n",
    "        if done:\n",
    "            \n",
    "            reward_list = agent.train()\n",
    "            avg.append(len(reward_list))\n",
    "            #print(len(reward_list), end=', ')\n",
    "            if len(avg) == 1000:\n",
    "                mean = np.mean(avg)\n",
    "                print(mean, end=', ')\n",
    "                avg = []\n",
    "                save_path = saver.save(agent.sess, './cart_models/model.ckpt')\n",
    "            break\n",
    "            \n",
    "        obs_prev = obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
