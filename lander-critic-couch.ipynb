{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================================#\n",
    "# Yes, this notebook is over-commented. #\n",
    "#=======================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make notebook span entire screen, horizontally.\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML('<style>.container { width:100% !important; }</style>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_rewards(rewards, decay, norm):\n",
    "    \"\"\"\n",
    "    Apply decay to raw rewards and maybe normalize.\n",
    "    \"\"\"\n",
    "    \n",
    "    discounted = np.zeros_like(rewards)\n",
    "    running_reward = 0\n",
    "        \n",
    "    for idx in reversed(range(len(rewards))):\n",
    "        running_reward += rewards[idx]\n",
    "        running_reward *= decay\n",
    "        discounted[idx] = running_reward\n",
    "            \n",
    "    if norm:\n",
    "        discounted -= np.mean(discounted)\n",
    "        if np.std(discounted) != 0:\n",
    "            discounted /= np.std(discounted)\n",
    "\n",
    "    return discounted.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(object):\n",
    "    def __init__(self, sess):\n",
    "        self.num_actions = 4\n",
    "        self.sess = sess\n",
    "        \n",
    "        self._build()\n",
    "        \n",
    "    def _build(self):\n",
    "        self.actions      = tf.placeholder(tf.int32, (None, 1))\n",
    "        self.columns      = tf.placeholder(tf.int32, (None, 1))\n",
    "        self.e_encr       = tf.placeholder(tf.float32)\n",
    "        self.l_rate       = tf.placeholder(tf.float32)\n",
    "        self.observations = tf.placeholder(tf.float32, (None, 8))\n",
    "        self.target       = tf.placeholder(tf.float32, (None, 1))\n",
    "        self.training     = tf.placeholder(tf.bool)\n",
    "        \n",
    "        with tf.variable_scope('actor-hidden'):\n",
    "            h1    = tf.layers.dense( \\\n",
    "                        self.observations,\n",
    "                        256, \n",
    "                        activation=tf.nn.relu, \n",
    "                        kernel_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                        name='h1')\n",
    "            \n",
    "            drop1 = tf.layers.dropout(h1, training=self.training, name='drop1')\n",
    "            \n",
    "            h2    = tf.layers.dense( \\\n",
    "                        drop1,\n",
    "                        128,\n",
    "                        activation=tf.nn.relu, \n",
    "                        kernel_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                        name='h2')\n",
    "            \n",
    "            drop2 = tf.layers.dropout(h2, training=self.training, name='drop2')\n",
    "            \n",
    "            h3    = tf.layers.dense( \\\n",
    "                        drop2,\n",
    "                        64,\n",
    "                        activation=tf.nn.relu, \n",
    "                        kernel_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                        name='h3')\n",
    "            \n",
    "            drop3 = tf.layers.dropout(h3, training=self.training, name='dropout')\n",
    "        \n",
    "            out   = tf.layers.dense( \\\n",
    "                        drop3,\n",
    "                        self.num_actions,\n",
    "                        kernel_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                        kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1),\n",
    "                        name='out')\n",
    "        \n",
    "        # Compute probabilities associated with each action. Clipping avoids\n",
    "        # passing 0 to log.\n",
    "        self.probabilities = tf.clip_by_value(tf.nn.softmax(out), 1e-10, 1.0)\n",
    "        \n",
    "        # Compute entropy of action probabilities.\n",
    "        self.entropy = -tf.reduce_sum(self.probabilities * tf.log(self.probabilities), 1, name=\"entropy\")\n",
    "        \n",
    "        # Compute punishment based on the difference between the min and max\n",
    "        # action probability to modulate action probability dominance.\n",
    "        min_max_punishment = tf.reduce_max(self.probabilities,axis=1) - tf.reduce_min(self.probabilities,axis=1)\n",
    "        min_max_weight = tf.reduce_max(out, axis=1) - tf.reduce_min(out, axis=1)\n",
    "        \n",
    "        # Collect probability of chosen action for each timestep.\n",
    "        indices = tf.concat(values=[self.columns, self.actions], axis=1)\n",
    "        self.picked_action_prob = tf.gather_nd(self.probabilities, indices)\n",
    "        \n",
    "        self.timestep_losses = ( \\\n",
    "            # Compute loss.\n",
    "            -tf.log(self.picked_action_prob) * self.target -\n",
    "            \n",
    "            # Incentivize entropy.\n",
    "            self.entropy * self.e_encr +\n",
    "            \n",
    "            # Disincentivize dominant action probabilities.\n",
    "            min_max_punishment * min_max_weight * 10 +\n",
    "            \n",
    "            # Regularize.\n",
    "            tf.losses.get_regularization_loss())\n",
    "        \n",
    "        # Compute batch loss.\n",
    "        self.loss = tf.reduce_mean(self.timestep_losses)\n",
    "        \n",
    "        # Set optimizer.\n",
    "        self.train_op = tf.train.AdamOptimizer(self.l_rate).minimize(self.loss)\n",
    "    \n",
    "    def choose_action(self, obs, verbose=False):\n",
    "        \"\"\"\n",
    "        Given an observation, use the policy network to determine what action to take.\n",
    "        \"\"\"\n",
    "        \n",
    "        debug = (self.entropy, self.loss)\n",
    "        \n",
    "        # Compute probabilities associated with each action.\n",
    "        probs, *results = self.sess.run((self.probabilities,) + (debug if verbose else ()), feed_dict={\n",
    "            self.observations: np.array(obs).reshape(-1, 8),\n",
    "            self.training:     False\n",
    "        })\n",
    "        \n",
    "        if verbose: print(probs, *results)\n",
    "            \n",
    "        # Choose action based on computed probabilities.\n",
    "        return np.random.choice(range(probs.shape[1]), p=probs.ravel())\n",
    "    \n",
    "    def train(self, act, obs, target, l_rate, e_encr, verbose):\n",
    "        \"\"\"\n",
    "        Train policy network on a timestep of data.\n",
    "        \"\"\"\n",
    "        \n",
    "        length = np.array(act).reshape(-1, 1).shape[0]\n",
    "        debug = (self.loss, self.entropy, self.probabilities)\n",
    "        \n",
    "        # Only store debugging info in results.\n",
    "        _, *results = self.sess.run( \\\n",
    "                          (self.train_op,) + (debug if verbose else ()),\n",
    "                          feed_dict = {\n",
    "                              self.actions:      np.array(act).reshape(-1, 1),\n",
    "                              self.columns:      np.arange(length).reshape(-1, 1),\n",
    "                              self.e_encr:       e_encr,\n",
    "                              self.l_rate:       l_rate,\n",
    "                              self.observations: np.array(obs).reshape(-1, 8),\n",
    "                              self.target:       np.array(target).reshape(-1, 1),\n",
    "                              self.training:     True\n",
    "                          })\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(object):\n",
    "    def __init__(self, sess):\n",
    "        self.sess = sess\n",
    "        \n",
    "        self._build()\n",
    "        \n",
    "    def _build(self):\n",
    "        self.l_rate       = tf.placeholder(tf.float32)\n",
    "        self.observations = tf.placeholder(tf.float32, (None, 8))\n",
    "        self.target       = tf.placeholder(tf.float32, (None, 1))\n",
    "        self.training     = tf.placeholder(tf.bool)\n",
    "        \n",
    "        with tf.variable_scope('critic-hidden'):\n",
    "            h1    = tf.layers.dense( \\\n",
    "                        self.observations,\n",
    "                        256,\n",
    "                        activation=tf.nn.relu,\n",
    "                        kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                        name='h1')\n",
    "            \n",
    "            drop1 = tf.layers.dropout(h1, name='drop1', training=self.training)\n",
    "            \n",
    "            h2    = tf.layers.dense( \\\n",
    "                        drop1,\n",
    "                        128,\n",
    "                        activation=tf.nn.relu,\n",
    "                        kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                        name='h2')\n",
    "            \n",
    "            drop2 = tf.layers.dropout(h2, name='drop2', training=self.training)\n",
    "            \n",
    "            out   = tf.layers.dense( \\\n",
    "                        drop2,\n",
    "                        1,\n",
    "                        kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                        name='out')\n",
    "        \n",
    "        # Output layer is a single node. Squeezing unpacks the value estimation\n",
    "        # from that node.\n",
    "        self.value_estimate = tf.squeeze(out)\n",
    "        \n",
    "        # Compute loss for each timestep in batch.\n",
    "        self.losses = tf.squared_difference(self.value_estimate, self.target)\n",
    "        \n",
    "        # Compute batch loss.\n",
    "        self.loss = tf.reduce_mean(self.losses)\n",
    "\n",
    "        # Set optimizer.\n",
    "        self.train_op = tf.train.AdamOptimizer(self.l_rate).minimize(self.loss)\n",
    "        \n",
    "    def predict(self, obs):\n",
    "        \"\"\"\n",
    "        Estimate value of current state of simulation (i.e. an observation).\n",
    "        \"\"\"\n",
    "        \n",
    "        return sess.run(self.value_estimate, feed_dict={\n",
    "            self.observations: np.array(obs).reshape(-1, 8),\n",
    "            self.training:     False\n",
    "        })\n",
    "    \n",
    "    def update(self, obs, target, l_rate, verbose):\n",
    "        \"\"\"\n",
    "        Train critic network on a timestep of data.\n",
    "        \"\"\"\n",
    "        \n",
    "        debug = (self.loss,)\n",
    "        \n",
    "        # Only store debugging info in results.\n",
    "        _, *results = self.sess.run( \\\n",
    "                          (self.train_op,) + (debug if verbose else ()),\n",
    "                          feed_dict = {\n",
    "                              self.l_rate:       l_rate,\n",
    "                              self.observations: np.array(obs).reshape(-1, 8),\n",
    "                              self.target:       np.array(target).reshape(-1, 1),\n",
    "                              self.training:     True\n",
    "                          })\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Handler(object):\n",
    "    def __init__(self, actor, critic, env, sess, path='./.model.ckpt'):\n",
    "        self.actor = actor\n",
    "        self.critic = critic\n",
    "        self.env = env\n",
    "        self.sess = sess\n",
    "    \n",
    "        self.saver = tf.train.Saver()\n",
    "        self.path = path\n",
    "\n",
    "    def init_vars(self):\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def run(self,\n",
    "            \n",
    "            # Specifies which training function to use.\n",
    "            train_fn_name,\n",
    "        \n",
    "            # Number of episodes per batch. Not used with batch-less training.\n",
    "            rollout  = 100,\n",
    "            \n",
    "            # Number of total episodes to train on. Not used with batch training.\n",
    "            episodes = 1000,\n",
    "            \n",
    "            # Learning rate of actor.\n",
    "            a_rate   = 0.001,\n",
    "            \n",
    "            # Learning rate of critic.\n",
    "            c_rate   = 0.005,\n",
    "            \n",
    "            # Weight with which to augment entropy-based punishment.\n",
    "            e_encr   = 0.007,\n",
    "            \n",
    "            # Reward decay.\n",
    "            decay    = 0.99,\n",
    "            \n",
    "            # Normalize rewards per episode?\n",
    "            norm     = False,\n",
    "            \n",
    "            # Render simulation to screen?\n",
    "            render   = False,\n",
    "            \n",
    "            # Print debugging info?\n",
    "            verbose  = False,\n",
    "            \n",
    "            # Hyperparameters specific to a training function.\n",
    "            **kwargs\n",
    "        ):\n",
    "        \n",
    "        \"\"\"\n",
    "        Run an arbitrary training function.\n",
    "        \n",
    "        The run method specifies defaults for hyperparameters common to all\n",
    "        training functions. Parameters specific to individual training functions\n",
    "        are passed along in kwargs.\n",
    "        \n",
    "        Episodes are only used for batch-less training and rollout is only used\n",
    "        for batch-based training.\n",
    "        \"\"\"\n",
    "        \n",
    "        # All training functions must be prefixed with 'train_'.\n",
    "        assert isinstance(train_fn_name, str) and train_fn_name.startswith('train_'), \\\n",
    "               'Invalid train_func name specified.'\n",
    "        \n",
    "        # Retreive training function.\n",
    "        train_fn = getattr(self, train_fn_name)\n",
    "        \n",
    "        # Provide total number of episodes to run for batch-less training\n",
    "        # or do rollout for batch training.\n",
    "        if train_fn_name == 'train_constant':\n",
    "            train_fn(episodes, render, a_rate, c_rate, decay, e_encr, verbose, **kwargs)\n",
    "        else:\n",
    "            train_fn(self.rollout(rollout, render, decay, norm), a_rate, c_rate, e_encr, verbose, **kwargs)\n",
    "        \n",
    "        # Close the display window.\n",
    "        if render: self.env.close()\n",
    "            \n",
    "    def train_constant(self,\n",
    "            \n",
    "            # Total number of episodes to train on.\n",
    "            num_episodes,\n",
    "                       \n",
    "            render,\n",
    "            \n",
    "            # Hyperparameters.\n",
    "            a_rate, c_rate, decay, e_encr,\n",
    "                       \n",
    "            verbose\n",
    "        ):\n",
    "        \n",
    "        \"\"\"\n",
    "        Run actor and train on every timestep.\n",
    "        \n",
    "        This training method does not use the default run method because run is\n",
    "        built for batched training and this method does not use batches.\n",
    "        \"\"\"\n",
    "\n",
    "        for ep in range(num_episodes):\n",
    "            obs_curr = env.reset()\n",
    "            done = False\n",
    "\n",
    "            if verbose:\n",
    "                a_episode_loss = []\n",
    "                c_episode_loss = []\n",
    "                rewards = 0\n",
    "            \n",
    "            while not done:\n",
    "                if render: self.env.render()\n",
    "                    \n",
    "                # Actor chooses action.\n",
    "                action = self.actor.choose_action(obs_curr)\n",
    "\n",
    "                # Actor takes action in environment.\n",
    "                next_obs, reward, done, _ = self.env.step(action)\n",
    "\n",
    "                # Critic estimates value of next state.\n",
    "                next_estimate = self.critic.predict(next_obs)\n",
    "                \n",
    "                # Compute value of current state assuming critic is accurate.\n",
    "                td_target = reward + decay * next_estimate\n",
    "                \n",
    "                # Compare indirect value estimation with direct value estimation of current state.\n",
    "                td_error = td_target - self.critic.predict(obs_curr)\n",
    "                \n",
    "                # Train actor and critic.\n",
    "                c_debug = self.critic.update(obs_curr, td_target, c_rate, verbose)\n",
    "                a_debug = self.actor.train(action, obs_curr, td_error, a_rate, e_encr, verbose)\n",
    "                \n",
    "                if verbose:\n",
    "                    # Get loss from debug info.\n",
    "                    a_episode_loss.append(a_debug[0])\n",
    "                    c_episode_loss.append(c_debug[0])\n",
    "                    \n",
    "                    rewards += reward\n",
    "\n",
    "                obs_curr = next_obs\n",
    "                \n",
    "            if verbose:\n",
    "                print('Actor Episode Loss: {:14.7f}'.format(np.mean(a_episode_loss)), end='; ')\n",
    "                print('Critic Episode Loss: {:14.7f}'.format(np.mean(c_episode_loss)), end='; ')\n",
    "                print('Episode Reward: {:14.7f}'.format(rewards))\n",
    "\n",
    "    def train_rsample(self,\n",
    "            \n",
    "            # Default batch-training parameters.\n",
    "            batch, a_rate, c_rate, e_encr, verbose,\n",
    "            \n",
    "            # Rounds of training to perform.\n",
    "            num_epochs      = 50,\n",
    "            \n",
    "            # Number of random timesteps to train on.\n",
    "            mini_batch_size = 100\n",
    "        ):\n",
    "        \n",
    "        \"\"\"\n",
    "        Performs random mini-batch training on both networks given a batch of\n",
    "        episode information.\n",
    "        \"\"\"\n",
    "        \n",
    "        if verbose:\n",
    "            a_batch_loss = []\n",
    "            c_batch_loss = []\n",
    "        \n",
    "        for _ in range(num_epochs):\n",
    "            # Randomly select timesteps to train on.\n",
    "            indices = np.random.randint(len(batch['obs']), size=mini_batch_size)\n",
    "            \n",
    "            a_debug = self.actor.train( \\\n",
    "                          # Batched episode information.\n",
    "                          [batch['act'][i] for i in indices],\n",
    "                          [batch['obs'][i] for i in indices],\n",
    "                          [batch['advantage'][i] for i in indices],\n",
    "                       \n",
    "                          # Hyperparameters.\n",
    "                          a_rate, e_encr,\n",
    "                                    \n",
    "                          verbose)\n",
    "            \n",
    "            c_debug = self.critic.update( \\\n",
    "                          # Batched episode information.\n",
    "                          [batch['obs'][i] for i in indices],\n",
    "                          [batch['td_target'][i] for i in indices],\n",
    "                          \n",
    "                          # Hyperparameters.\n",
    "                          c_rate,\n",
    "                          \n",
    "                          verbose)\n",
    "            \n",
    "            if verbose:\n",
    "                a_batch_loss.append(a_debug[0])\n",
    "                c_batch_loss.append(c_debug[0])\n",
    "            \n",
    "        if verbose:\n",
    "            print('Actor Batch Loss: {:14.7f}'.format(np.mean(a_batch_loss)), end='; ')\n",
    "            print('Critic Batch Loss: {:14.7f}'.format(np.mean(a_batch_loss)), end='; ')\n",
    "            print('Batch Reward: {:14.7f}'.format(batch['avg_rew']))\n",
    " \n",
    "    def train_all(self,\n",
    "\n",
    "            # Default batch-training parameters.\n",
    "            batch, a_rate, c_rate, e_encr, verbose\n",
    "        ):\n",
    "        \n",
    "        \"\"\"\n",
    "        Trains both networks on the entirety of a batch of episode information.\n",
    "        \"\"\"\n",
    "        \n",
    "        a_debug = self.actor.train( \\\n",
    "                      # Batched episode information.\n",
    "                      batch['act'],\n",
    "                      batch['obs'],\n",
    "                      batch['advantage'],\n",
    "                      \n",
    "                      # Hyperparameters.\n",
    "                      a_rate, e_encr,\n",
    "                      \n",
    "                      verbose)\n",
    "        \n",
    "        c_debug = self.critic.update( \\\n",
    "                      # Batched episode information.\n",
    "                      batch['obs'],\n",
    "                      batch['td_target'],\n",
    "                      \n",
    "                      # Hyperparameters.\n",
    "                      c_rate,\n",
    "                      \n",
    "                      verbose)\n",
    "        \n",
    "        if verbose:\n",
    "            print('Actor Batch Loss: {:14.7f}'.format(a_debug[0]), end='; ')\n",
    "            print('Critic Batch Loss: {:14.7f}'.format(c_debug[0]), end='; ')\n",
    "            print('Batch Reward: {:14.7f}'.format(batch['avg_rew']))\n",
    "    \n",
    "    def compute_advantage(self, obs, rewards, decay, norm):\n",
    "        # Discount and maybe normalize rewards.\n",
    "        disc_rewards = process_rewards(rewards, decay, norm)\n",
    "\n",
    "        policy_target = np.zeros_like(disc_rewards)\n",
    "        value_target = np.zeros_like(disc_rewards)\n",
    "        running_reward = 0\n",
    "\n",
    "        for idx in range(len(disc_rewards)):\n",
    "            # Critic estimates processed value of current state.\n",
    "            estimate = self.critic.predict(obs[idx])\n",
    "            \n",
    "            # Target for critic is actual processed value.\n",
    "            td_target = disc_rewards[idx]\n",
    "            value_target[idx] = td_target\n",
    "            \n",
    "            # Advantage for actor is error in critic's estimation.\n",
    "            td_error = td_target - estimate\n",
    "            policy_target[idx] = td_error\n",
    "        \n",
    "        return policy_target.tolist(), value_target.tolist()    \n",
    "\n",
    "    def save(self):\n",
    "        self.saver.save(self.sess, self.path)\n",
    "        \n",
    "    def restore(self):\n",
    "        self.saver.restore(self.sess, self.path)\n",
    "        \n",
    "    def play(self, verbose=False):\n",
    "        \"\"\"\n",
    "        Runs a single instance of the game without training or storing training\n",
    "        information. Always displays the game and closes the window afterward.\n",
    "        \"\"\"\n",
    "        \n",
    "        obs_curr = self.env.reset()\n",
    "        done = False\n",
    "        \n",
    "        if verbose: rewards = 0\n",
    "        \n",
    "        while not done:\n",
    "            self.env.render()\n",
    "\n",
    "            # Actor chooses action.\n",
    "            action = self.actor.choose_action(obs_curr, verbose=verbose)\n",
    "\n",
    "            # Actor takes action in environment.\n",
    "            obs_curr, reward, done, _ = self.env.step(action)\n",
    "            \n",
    "            if verbose: rewards += reward\n",
    "            \n",
    "        if verbose: print('Episode Reward: {:14.7f}'.format(rewards))\n",
    "        self.env.close()\n",
    "            \n",
    "    def rollout(self, count, render, decay, norm):\n",
    "        # Holds information about entire rollout of episodes.\n",
    "        batch = {'act': [], 'obs': [], 'rew': [], 'advantage':[], 'td_target':[]}\n",
    "        \n",
    "        rewards = 0\n",
    "        \n",
    "        for episode in range(count):\n",
    "            # Episode batch for current episode to be appended to rollout batch.\n",
    "            history = {'act': [], 'obs': [], 'rew': [], 'advantage':[], 'td_target':[]}\n",
    "            \n",
    "            obs_curr = env.reset()\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                if render: self.env.render()\n",
    "                \n",
    "                # Actor chooses action.\n",
    "                action = self.actor.choose_action(obs_curr, False)\n",
    "        \n",
    "                # Take action in environment.\n",
    "                next_obs, reward, done, _ = self.env.step(action)\n",
    "                \n",
    "                history['act'].append(action)\n",
    "                history['obs'].append(obs_curr)\n",
    "                history['rew'].append(reward)\n",
    "                \n",
    "                rewards += reward\n",
    "                \n",
    "                obs_curr = next_obs\n",
    "\n",
    "            ( # Preprocess episode information for training.\n",
    "                history['advantage'],\n",
    "                history['td_target']\n",
    "            ) = self.compute_advantage(history['obs'] + obs_curr, history['rew'], decay, norm)\n",
    "            \n",
    "            # Add episode to batch.\n",
    "            for key in batch:\n",
    "                batch[key].extend(history[key])\n",
    "                \n",
    "        # Include average reward of rollout batch.\n",
    "        batch['avg_rew'] = rewards / count\n",
    "        \n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "actor = Actor(sess)\n",
    "critic = Critic(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = Handler(actor, critic, env, sess, '.models/l1.cpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler.init_vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(True):\n",
    "    for _ in range(100):\n",
    "        handler.run('train_all', rollout=40, a_rate=0.001, c_rate=0.005, decay=0.99, render=False, verbose=True)\n",
    "        handler.play()\n",
    "    print('Completed 100 Training Iterations\\n')\n",
    "    handler.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(True):\n",
    "    for _ in range(100):\n",
    "        handler.run('train_rsample', verbose=True)\n",
    "    print('\\nCompleted 100 Training Iterations\\n')\n",
    "    handler.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler.run('train_constant', episodes=200, decay=0.99, a_rate=0.002, c_rate=0.01, e_encr=0.008, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler.restore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while True: handler.play(verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
