{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================================#\n",
    "# Yes, this notebook is over-commented. #\n",
    "#=======================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make notebook span entire screen, horizontally.\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML('<style>.container { width:100% !important; }</style>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_rewards(rewards, decay, norm):\n",
    "    \"\"\"\n",
    "    Apply decay to raw rewards and maybe normalize.\n",
    "    \"\"\"\n",
    "    \n",
    "    discounted = np.zeros_like(rewards)\n",
    "    running_reward = 0\n",
    "        \n",
    "    for idx in reversed(range(len(rewards))):\n",
    "        running_reward += rewards[idx]\n",
    "        running_reward *= decay\n",
    "        discounted[idx] = running_reward\n",
    "            \n",
    "    if norm:\n",
    "        discounted -= np.mean(discounted)\n",
    "        if np.std(discounted) != 0:\n",
    "            discounted /= np.std(discounted)\n",
    "\n",
    "    return discounted.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(object):\n",
    "    def __init__(self, sess):\n",
    "        self.num_actions = 4\n",
    "        self.sess = sess\n",
    "        \n",
    "        self._build()\n",
    "        \n",
    "    def _build(self):\n",
    "        self.actions      = tf.placeholder(tf.int32, (None, 1))\n",
    "        self.columns      = tf.placeholder(tf.int32, (None, 1))\n",
    "        self.e_encr       = tf.placeholder(tf.float32)\n",
    "        self.l_rate       = tf.placeholder(tf.float32)\n",
    "        self.min_max_w    = tf.placeholder(tf.float32)\n",
    "        self.observations = tf.placeholder(tf.float32, (None, 8))\n",
    "        self.target       = tf.placeholder(tf.float32, (None, 1))\n",
    "        self.training     = tf.placeholder(tf.bool)\n",
    "        \n",
    "        with tf.variable_scope('actor-hidden'):\n",
    "            h1    = tf.layers.dense( \\\n",
    "                        self.observations,\n",
    "                        256, \n",
    "                        activation=tf.nn.relu, \n",
    "                        kernel_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                        name='h1')\n",
    "            \n",
    "            drop1 = tf.layers.dropout(h1, training=self.training, name='drop1')\n",
    "            \n",
    "            h2    = tf.layers.dense( \\\n",
    "                        drop1,\n",
    "                        128,\n",
    "                        activation=tf.nn.relu, \n",
    "                        kernel_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                        name='h2')\n",
    "            \n",
    "            drop2 = tf.layers.dropout(h2, training=self.training, name='drop2')\n",
    "            \n",
    "            h3    = tf.layers.dense( \\\n",
    "                        drop2,\n",
    "                        64,\n",
    "                        activation=tf.nn.relu, \n",
    "                        kernel_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                        name='h3')\n",
    "            \n",
    "            drop3 = tf.layers.dropout(h3, training=self.training, name='dropout')\n",
    "        \n",
    "            out   = tf.layers.dense( \\\n",
    "                        drop3,\n",
    "                        self.num_actions,\n",
    "                        kernel_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                        kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1),\n",
    "                        name='out')\n",
    "        \n",
    "        # Compute probabilities associated with each action. Clipping avoids\n",
    "        # passing 0 to log.\n",
    "        self.probabilities = tf.clip_by_value(tf.nn.softmax(out), 1e-10, 1.0)\n",
    "        \n",
    "        # Compute entropy of action probabilities.\n",
    "        self.entropy = -tf.reduce_sum(self.probabilities * tf.log(self.probabilities), 1, name=\"entropy\")\n",
    "        \n",
    "        # Compute punishment based on the difference between the min and max\n",
    "        # action probability to modulate action probability dominance.\n",
    "        min_max_punishment = tf.reduce_max(self.probabilities,axis=1) - tf.reduce_min(self.probabilities,axis=1)\n",
    "        \n",
    "        # Collect probability of chosen action for each timestep.\n",
    "        indices = tf.concat(values=[self.columns, self.actions], axis=1)\n",
    "        self.picked_action_prob = tf.gather_nd(self.probabilities, indices)\n",
    "        \n",
    "        self.timestep_losses = ( \\\n",
    "            # Compute loss.\n",
    "            -tf.log(self.picked_action_prob) * self.target -\n",
    "            \n",
    "            # Incentivize entropy.\n",
    "            self.entropy * self.e_encr +\n",
    "            \n",
    "            # Disincentivize dominant action probabilities.\n",
    "            min_max_punishment * self.min_max_w +\n",
    "            \n",
    "            # Regularize.\n",
    "            tf.losses.get_regularization_loss())\n",
    "        \n",
    "        # Compute batch loss.\n",
    "        self.loss = tf.reduce_mean(self.timestep_losses)\n",
    "        \n",
    "        # Set optimizer.\n",
    "        self.train_op = tf.train.AdamOptimizer(self.l_rate).minimize(self.loss)\n",
    "    \n",
    "    def choose_action(self, obs, verbose=False):\n",
    "        \"\"\"\n",
    "        Given an observation, use the policy network to determine what action to take.\n",
    "        \"\"\"\n",
    "        \n",
    "        debug = (self.entropy, )\n",
    "        \n",
    "        # Compute probabilities associated with each action.\n",
    "        probs, *results = self.sess.run((self.probabilities,) + (debug if verbose else ()), feed_dict={\n",
    "            self.observations: np.array(obs).reshape(-1, 8),\n",
    "            self.training:     False\n",
    "        })\n",
    "        \n",
    "        if verbose: print(probs, *results)\n",
    "            \n",
    "        # Choose action based on computed probabilities.\n",
    "        return np.random.choice(range(probs.shape[1]), p=probs.ravel())\n",
    "    \n",
    "    def train(self, act, obs, target, l_rate, e_encr, min_max_w, verbose):\n",
    "        \"\"\"\n",
    "        Train policy network on a timestep of data.\n",
    "        \"\"\"\n",
    "        \n",
    "        length = np.array(act).reshape(-1, 1).shape[0]\n",
    "        debug = (self.loss, self.entropy, self.probabilities)\n",
    "        \n",
    "        # Only store debugging info in results.\n",
    "        _, *results = self.sess.run( \\\n",
    "                          (self.train_op,) + (debug if verbose else ()),\n",
    "                          feed_dict = {\n",
    "                              self.actions:      np.array(act).reshape(-1, 1),\n",
    "                              self.columns:      np.arange(length).reshape(-1, 1),\n",
    "                              self.e_encr:       e_encr,\n",
    "                              self.l_rate:       l_rate,\n",
    "                              self.observations: np.array(obs).reshape(-1, 8),\n",
    "                              self.target:       np.array(target).reshape(-1, 1),\n",
    "                              self.training:     True,\n",
    "                              self.min_max_w:    min_max_w\n",
    "                          })\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(object):\n",
    "    def __init__(self, sess):\n",
    "        self.sess = sess\n",
    "        \n",
    "        self._build()\n",
    "        \n",
    "    def _build(self):\n",
    "        self.l_rate       = tf.placeholder(tf.float32)\n",
    "        self.observations = tf.placeholder(tf.float32, (None, 8))\n",
    "        self.target       = tf.placeholder(tf.float32, (None, 1))\n",
    "        self.training     = tf.placeholder(tf.bool)\n",
    "        \n",
    "        with tf.variable_scope('critic-hidden'):\n",
    "            h1    = tf.layers.dense( \\\n",
    "                        self.observations,\n",
    "                        256,\n",
    "                        activation=tf.nn.relu,\n",
    "                        kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                        name='h1')\n",
    "            \n",
    "            drop1 = tf.layers.dropout(h1, name='drop1', training=self.training)\n",
    "            \n",
    "            h2    = tf.layers.dense( \\\n",
    "                        drop1,\n",
    "                        128,\n",
    "                        activation=tf.nn.relu,\n",
    "                        kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                        name='h2')\n",
    "            \n",
    "            drop2 = tf.layers.dropout(h2, name='drop2', training=self.training)\n",
    "            \n",
    "            out   = tf.layers.dense( \\\n",
    "                        drop2,\n",
    "                        1,\n",
    "                        kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                        name='out')\n",
    "        \n",
    "        # Output layer is a single node. Squeezing unpacks the value estimation\n",
    "        # from that node.\n",
    "        self.value_estimate = tf.squeeze(out)\n",
    "        \n",
    "        # Compute loss for each timestep in batch.\n",
    "        self.losses = tf.squared_difference(self.value_estimate, self.target)\n",
    "        \n",
    "        # Compute batch loss.\n",
    "        self.loss = tf.reduce_mean(self.losses)\n",
    "\n",
    "        # Set optimizer.\n",
    "        self.train_op = tf.train.AdamOptimizer(self.l_rate).minimize(self.loss)\n",
    "        \n",
    "    def predict(self, obs):\n",
    "        \"\"\"\n",
    "        Estimate value of current state of simulation (i.e. an observation).\n",
    "        \"\"\"\n",
    "        \n",
    "        return sess.run(self.value_estimate, feed_dict={\n",
    "            self.observations: np.array(obs).reshape(-1, 8),\n",
    "            self.training:     False\n",
    "        })\n",
    "    \n",
    "    def update(self, obs, target, l_rate, verbose):\n",
    "        \"\"\"\n",
    "        Train critic network on a timestep of data.\n",
    "        \"\"\"\n",
    "        \n",
    "        debug = (self.loss,)\n",
    "        \n",
    "        # Only store debugging info in results.\n",
    "        _, *results = self.sess.run( \\\n",
    "                          (self.train_op,) + (debug if verbose else ()),\n",
    "                          feed_dict = {\n",
    "                              self.l_rate:       l_rate,\n",
    "                              self.observations: np.array(obs).reshape(-1, 8),\n",
    "                              self.target:       np.array(target).reshape(-1, 1),\n",
    "                              self.training:     True\n",
    "                          })\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Handler(object):\n",
    "    def __init__(self, actor, critic, env, sess, path='./.model.ckpt'):\n",
    "        self.actor = actor\n",
    "        self.critic = critic\n",
    "        self.env = env\n",
    "        self.sess = sess\n",
    "    \n",
    "        self.saver = tf.train.Saver()\n",
    "        self.path = path\n",
    "\n",
    "    def init_vars(self):\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def run(self,\n",
    "            \n",
    "            # Specifies which training function to use.\n",
    "            train_fn_name,\n",
    "        \n",
    "            # Number of episodes per batch. Not used with batch-less training.\n",
    "            rollout  = 100,\n",
    "            \n",
    "            # Number of total episodes to train on. Not used with batch training.\n",
    "            episodes = 1000,\n",
    "            \n",
    "            # Learning rate of actor.\n",
    "            a_rate   = 0.001,\n",
    "            \n",
    "            # Learning rate of critic.\n",
    "            c_rate   = 0.005,\n",
    "            \n",
    "            # Weight with which to augment entropy-based encouragement.\n",
    "            e_encr   = 0.007,\n",
    "            \n",
    "            # Reward decay.\n",
    "            decay    = 0.99,\n",
    "            \n",
    "            # Weight to augment punishment based on maximum and minimum\n",
    "            # ouput probability\n",
    "            min_max_w = 1,\n",
    "            \n",
    "            # Normalize rewards per episode?\n",
    "            norm     = False,\n",
    "            \n",
    "            # Render simulation to screen?\n",
    "            render   = False,\n",
    "            \n",
    "            # Print debugging info?\n",
    "            verbose  = False,\n",
    "            \n",
    "            # Hyperparameters specific to a training function.\n",
    "            **kwargs\n",
    "        ):\n",
    "        \n",
    "        \"\"\"\n",
    "        Run an arbitrary training function.\n",
    "        \n",
    "        The run method specifies defaults for hyperparameters common to all\n",
    "        training functions. Parameters specific to individual training functions\n",
    "        are passed along in kwargs.\n",
    "        \n",
    "        Episodes are only used for batch-less training and rollout is only used\n",
    "        for batch-based training.\n",
    "        \"\"\"\n",
    "        \n",
    "        # All training functions must be prefixed with 'train_'.\n",
    "        assert isinstance(train_fn_name, str) and train_fn_name.startswith('train_'), \\\n",
    "               'Invalid train_func name specified.'\n",
    "        \n",
    "        # Retreive training function.\n",
    "        train_fn = getattr(self, train_fn_name)\n",
    "        \n",
    "        # Provide total number of episodes to run for batch-less training\n",
    "        # or do rollout for batch training.\n",
    "        if train_fn_name == 'train_constant':\n",
    "            train_fn(episodes, render, a_rate, c_rate, decay, e_encr, min_max_w, verbose, **kwargs)\n",
    "        else:\n",
    "            train_fn(self.rollout(rollout, render, decay, norm), a_rate, c_rate, e_encr, min_max_w, verbose, **kwargs)\n",
    "        \n",
    "        # Close the display window.\n",
    "        if render: self.env.close()\n",
    "            \n",
    "    def train_constant(self,\n",
    "            \n",
    "            # Total number of episodes to train on.\n",
    "            num_episodes,\n",
    "                       \n",
    "            render,\n",
    "            \n",
    "            # Hyperparameters.\n",
    "            a_rate, c_rate, decay, e_encr, min_max_w,\n",
    "                       \n",
    "            verbose\n",
    "        ):\n",
    "        \n",
    "        \"\"\"\n",
    "        Run actor and train on every timestep.\n",
    "        \n",
    "        This training method does not use the default run method because run is\n",
    "        built for batched training and this method does not use batches.\n",
    "        \"\"\"\n",
    "\n",
    "        for ep in range(num_episodes):\n",
    "            obs_curr = env.reset()\n",
    "            done = False\n",
    "\n",
    "            if verbose:\n",
    "                a_episode_loss = []\n",
    "                c_episode_loss = []\n",
    "                rewards = 0\n",
    "            \n",
    "            while not done:\n",
    "                if render: self.env.render()\n",
    "                    \n",
    "                # Actor chooses action.\n",
    "                action = self.actor.choose_action(obs_curr)\n",
    "\n",
    "                # Actor takes action in environment.\n",
    "                next_obs, reward, done, _ = self.env.step(action)\n",
    "\n",
    "                # Critic estimates value of next state.\n",
    "                next_estimate = self.critic.predict(next_obs)\n",
    "                \n",
    "                # Compute value of current state assuming critic is accurate.\n",
    "                td_target = reward + decay * next_estimate\n",
    "                \n",
    "                # Compare indirect value estimation with direct value estimation of current state.\n",
    "                td_error = td_target - self.critic.predict(obs_curr)\n",
    "                \n",
    "                # Train actor and critic.\n",
    "                c_debug = self.critic.update(obs_curr, td_target, c_rate, verbose)\n",
    "                a_debug = self.actor.train(action, obs_curr, td_error, a_rate, e_encr, min_max_w, verbose)\n",
    "                \n",
    "                if verbose:\n",
    "                    # Get loss from debug info.\n",
    "                    a_episode_loss.append(a_debug[0])\n",
    "                    c_episode_loss.append(c_debug[0])\n",
    "                    \n",
    "                    rewards += reward\n",
    "\n",
    "                obs_curr = next_obs\n",
    "                \n",
    "            if verbose:\n",
    "                print('Actor Episode Loss: {:14.7f}'.format(np.mean(a_episode_loss)), end='; ')\n",
    "                print('Critic Episode Loss: {:14.7f}'.format(np.mean(c_episode_loss)), end='; ')\n",
    "                print('Episode Reward: {:14.7f}'.format(rewards))\n",
    "\n",
    "    def train_rsample(self,\n",
    "            \n",
    "            # Default batch-training parameters.\n",
    "            batch, a_rate, c_rate, e_encr, min_max_w, verbose,\n",
    "            \n",
    "            # Rounds of training to perform.\n",
    "            num_epochs      = 50,\n",
    "            \n",
    "            # Number of random timesteps to train on.\n",
    "            mini_batch_size = 100\n",
    "        ):\n",
    "        \n",
    "        \"\"\"\n",
    "        Performs random mini-batch training on both networks given a batch of\n",
    "        episode information.\n",
    "        \"\"\"\n",
    "        \n",
    "        if verbose:\n",
    "            a_batch_loss = []\n",
    "            c_batch_loss = []\n",
    "        \n",
    "        for _ in range(num_epochs):\n",
    "            # Randomly select timesteps to train on.\n",
    "            indices = np.random.randint(len(batch['obs']), size=mini_batch_size)\n",
    "            \n",
    "            a_debug = self.actor.train( \\\n",
    "                          # Batched episode information.\n",
    "                          [batch['act'][i] for i in indices],\n",
    "                          [batch['obs'][i] for i in indices],\n",
    "                          [batch['advantage'][i] for i in indices],\n",
    "                       \n",
    "                          # Hyperparameters.\n",
    "                          a_rate, e_encr, min_max_w,\n",
    "                                    \n",
    "                          verbose)\n",
    "            \n",
    "            c_debug = self.critic.update( \\\n",
    "                          # Batched episode information.\n",
    "                          [batch['obs'][i] for i in indices],\n",
    "                          [batch['td_target'][i] for i in indices],\n",
    "                          \n",
    "                          # Hyperparameters.\n",
    "                          c_rate,\n",
    "                          \n",
    "                          verbose)\n",
    "            \n",
    "            if verbose:\n",
    "                a_batch_loss.append(a_debug[0])\n",
    "                c_batch_loss.append(c_debug[0])\n",
    "            \n",
    "        if verbose:\n",
    "            print('Actor Batch Loss: {:14.7f}'.format(np.mean(a_batch_loss)), end='; ')\n",
    "            print('Critic Batch Loss: {:14.7f}'.format(np.mean(a_batch_loss)), end='; ')\n",
    "            print('Batch Reward: {:14.7f}'.format(batch['avg_rew']))\n",
    " \n",
    "    def train_all(self,\n",
    "\n",
    "            # Default batch-training parameters.\n",
    "            batch, a_rate, c_rate, e_encr, min_max_w, verbose\n",
    "        ):\n",
    "        \n",
    "        \"\"\"\n",
    "        Trains both networks on the entirety of a batch of episode information.\n",
    "        \"\"\"\n",
    "        \n",
    "        a_debug = self.actor.train( \\\n",
    "                      # Batched episode information.\n",
    "                      batch['act'],\n",
    "                      batch['obs'],\n",
    "                      batch['advantage'],\n",
    "                      \n",
    "                      # Hyperparameters.\n",
    "                      a_rate, e_encr, min_max_w,\n",
    "                      \n",
    "                      verbose)\n",
    "        \n",
    "        c_debug = self.critic.update( \\\n",
    "                      # Batched episode information.\n",
    "                      batch['obs'],\n",
    "                      batch['td_target'],\n",
    "                      \n",
    "                      # Hyperparameters.\n",
    "                      c_rate,\n",
    "                      \n",
    "                      verbose)\n",
    "        \n",
    "        if verbose:\n",
    "            print('Actor Batch Loss: {:14.7f}'.format(a_debug[0]), end='; ')\n",
    "            print('Critic Batch Loss: {:14.7f}'.format(c_debug[0]), end='; ')\n",
    "            print('Batch Reward: {:14.7f}'.format(batch['avg_rew']))\n",
    "    \n",
    "    def compute_advantage(self, obs, rewards, decay, norm):\n",
    "        # Discount and maybe normalize rewards.\n",
    "        disc_rewards = process_rewards(rewards, decay, norm)\n",
    "\n",
    "        policy_target = np.zeros_like(disc_rewards)\n",
    "        value_target = np.zeros_like(disc_rewards)\n",
    "        running_reward = 0\n",
    "\n",
    "        for idx in range(len(disc_rewards)):\n",
    "            # Critic estimates processed value of current state.\n",
    "            estimate = self.critic.predict(obs[idx])\n",
    "            \n",
    "            # Target for critic is actual processed value.\n",
    "            td_target = disc_rewards[idx]\n",
    "            value_target[idx] = td_target\n",
    "            \n",
    "            # Advantage for actor is error in critic's estimation.\n",
    "            td_error = td_target - estimate\n",
    "            policy_target[idx] = td_error\n",
    "        \n",
    "        return policy_target.tolist(), value_target.tolist()    \n",
    "\n",
    "    def save(self):\n",
    "        self.saver.save(self.sess, self.path)\n",
    "        \n",
    "    def restore(self):\n",
    "        self.saver.restore(self.sess, self.path)\n",
    "        \n",
    "    def play(self, verbose=False):\n",
    "        \"\"\"\n",
    "        Runs a single instance of the game without training or storing training\n",
    "        information. Always displays the game and closes the window afterward.\n",
    "        \"\"\"\n",
    "        \n",
    "        obs_curr = self.env.reset()\n",
    "        done = False\n",
    "        \n",
    "        if verbose: rewards = 0\n",
    "        \n",
    "        while not done:\n",
    "            self.env.render()\n",
    "\n",
    "            # Actor chooses action.\n",
    "            action = self.actor.choose_action(obs_curr, verbose=verbose)\n",
    "\n",
    "            # Actor takes action in environment.\n",
    "            obs_curr, reward, done, _ = self.env.step(action)\n",
    "            \n",
    "            if verbose: rewards += reward\n",
    "            \n",
    "        if verbose: print('Episode Reward: {:14.7f}'.format(rewards))\n",
    "        self.env.close()\n",
    "            \n",
    "    def rollout(self, count, render, decay, norm):\n",
    "        # Holds information about entire rollout of episodes.\n",
    "        batch = {'act': [], 'obs': [], 'rew': [], 'advantage':[], 'td_target':[]}\n",
    "        \n",
    "        rewards = 0\n",
    "        \n",
    "        for episode in range(count):\n",
    "            # Episode batch for current episode to be appended to rollout batch.\n",
    "            history = {'act': [], 'obs': [], 'rew': [], 'advantage':[], 'td_target':[]}\n",
    "            \n",
    "            obs_curr = env.reset()\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                if render: self.env.render()\n",
    "                \n",
    "                # Actor chooses action.\n",
    "                action = self.actor.choose_action(obs_curr, False)\n",
    "        \n",
    "                # Take action in environment.\n",
    "                next_obs, reward, done, _ = self.env.step(action)\n",
    "                \n",
    "                history['act'].append(action)\n",
    "                history['obs'].append(obs_curr)\n",
    "                history['rew'].append(reward)\n",
    "                \n",
    "                rewards += reward\n",
    "                \n",
    "                obs_curr = next_obs\n",
    "\n",
    "            ( # Preprocess episode information for training.\n",
    "                history['advantage'],\n",
    "                history['td_target']\n",
    "            ) = self.compute_advantage(history['obs'] + obs_curr, history['rew'], decay, norm)\n",
    "            \n",
    "            # Add episode to batch.\n",
    "            for key in batch:\n",
    "                batch[key].extend(history[key])\n",
    "                \n",
    "        # Include average reward of rollout batch.\n",
    "        batch['avg_rew'] = rewards / count\n",
    "        \n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "actor = Actor(sess)\n",
    "critic = Critic(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = Handler(actor, critic, env, sess, '.models/l1.cpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler.init_vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-a1873a5d1ad7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mhandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_all'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrollout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mhandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Completed 100 Training Iterations\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-ca3e023104ed>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, train_fn_name, rollout, episodes, a_rate, c_rate, e_encr, decay, min_max_w, norm, render, verbose, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_encr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_max_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrollout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_encr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_max_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# Close the display window.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-ca3e023104ed>\u001b[0m in \u001b[0;36mrollout\u001b[0;34m(self, count, render, decay, norm)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'advantage'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m                 \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'td_target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m             ) = self.compute_advantage(history['obs'] + obs_curr, history['rew'], decay, norm)\n\u001b[0m\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;31m# Add episode to batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-ca3e023104ed>\u001b[0m in \u001b[0;36mcompute_advantage\u001b[0;34m(self, obs, rewards, decay, norm)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisc_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0;31m# Critic estimates processed value of current state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             \u001b[0mestimate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0;31m# Target for critic is actual processed value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-a610861910e8>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     53\u001b[0m         return sess.run(self.value_estimate, feed_dict={\n\u001b[1;32m     54\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m     \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         })\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ind_study_project/.env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ind_study_project/.env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ind_study_project/.env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ind_study_project/.env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ind_study_project/.env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ind_study_project/.env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "    for _ in range(100):\n",
    "        handler.run('train_all', rollout=40, a_rate=0.001, c_rate=0.005, decay=0.99, render=False, verbose=True)\n",
    "        handler.play()\n",
    "    print('Completed 100 Training Iterations\\n')\n",
    "    handler.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(True):\n",
    "    for _ in range(100):\n",
    "        handler.run('train_rsample', verbose=True)\n",
    "    print('\\nCompleted 100 Training Iterations\\n')\n",
    "    handler.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor Episode Loss:      4.7153816; Critic Episode Loss:     96.9322586; Episode Reward:    -32.3958694\n",
      "Actor Episode Loss:    -12.0564423; Critic Episode Loss:    253.6167450; Episode Reward:   -223.9823998\n",
      "Actor Episode Loss:    -17.2791958; Critic Episode Loss:     78.2390747; Episode Reward:   -296.8419335\n",
      "Actor Episode Loss:    -16.1930866; Critic Episode Loss:    127.4874725; Episode Reward:   -245.9365018\n",
      "Actor Episode Loss:    -48.4692917; Critic Episode Loss:    157.5482025; Episode Reward:   -419.2858078\n",
      "Actor Episode Loss:    -11.7122879; Critic Episode Loss:     60.8506737; Episode Reward:   -286.5809250\n",
      "Actor Episode Loss:     -3.6929219; Critic Episode Loss:     63.8653831; Episode Reward:   -263.5226889\n",
      "Actor Episode Loss:    -88.1345520; Critic Episode Loss:    112.0604782; Episode Reward:   -268.3185656\n",
      "Actor Episode Loss:   -273.0624695; Critic Episode Loss:   2933.2211914; Episode Reward:   -415.7032026\n",
      "Actor Episode Loss:    -15.4538879; Critic Episode Loss:    450.2093201; Episode Reward:    -25.0266725\n",
      "Actor Episode Loss:   -116.7873383; Critic Episode Loss:    152.4772797; Episode Reward:   -609.1397395\n",
      "Actor Episode Loss:      2.0980959; Critic Episode Loss:     49.2778854; Episode Reward:    -95.7298394\n",
      "Actor Episode Loss:    -63.6158905; Critic Episode Loss:    252.6855011; Episode Reward:   -661.6583068\n",
      "Actor Episode Loss:    -39.9273491; Critic Episode Loss:    588.5534668; Episode Reward:   -286.6633731\n",
      "Actor Episode Loss:    -81.9924545; Critic Episode Loss:    299.1405334; Episode Reward:   -561.2229752\n"
     ]
    }
   ],
   "source": [
    "handler.run('train_constant', episodes=200, decay=0.98, a_rate=0.002, c_rate=0.01, e_encr=0.005, min_max_w=1, verbose=True, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler.restore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while True: handler.play(verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
