{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================================#\n",
    "# Yes, this notebook is over-commented. #\n",
    "#=======================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make notebook span entire screen, horizontally.\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyAgent(object):\n",
    "    def __init__(self, sess):\n",
    "        self.num_actions = 4\n",
    "        self._build()\n",
    "        self.sess = sess\n",
    "        \n",
    "    def _build(self):\n",
    "        self.actions      = tf.placeholder(tf.int32, (None, 1))\n",
    "        self.columns      = tf.placeholder(tf.int32, (None, 1))\n",
    "        self.e_weight     = tf.placeholder(tf.float32)\n",
    "        self.l_rate       = tf.placeholder(tf.float32)\n",
    "        self.observations = tf.placeholder(tf.float32, (None, 8))\n",
    "        self.target       = tf.placeholder(tf.float32, (None, 1))\n",
    "        self.training     = tf.placeholder(tf.bool)\n",
    "        \n",
    "        with tf.variable_scope('actor-hidden'):\n",
    "            h1    = tf.layers.dense(self.observations, 128, \n",
    "                                    activation=tf.nn.relu, \n",
    "                                    kernel_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                    name='h1')\n",
    "            \n",
    "            drop1 = tf.layers.dropout(h1, training=self.training, name='drop1')\n",
    "            \n",
    "            h2    = tf.layers.dense(drop1, 64,\n",
    "                                    activation=tf.nn.relu, \n",
    "                                    kernel_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                    name='h2')\n",
    "            \n",
    "            drop2 = tf.layers.dropout(h2, training=self.training, name='drop2')\n",
    "            \n",
    "            h3    = tf.layers.dense(drop2, 32,\n",
    "                                    activation=tf.nn.relu, \n",
    "                                    kernel_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                    name='h3')\n",
    "            \n",
    "            drop3 = tf.layers.dropout(h3, training=self.training, name='dropout')\n",
    "        \n",
    "            self.out = tf.layers.dense(drop3, self.num_actions,\n",
    "                                       kernel_initializer=tf.random_normal_initializer(), \n",
    "                                       name='out')\n",
    "        \n",
    "        # Compute probabilities associated with each action.\n",
    "        self.probabilities = tf.clip_by_value(tf.nn.softmax(self.out), 1e-10, 1.0)\n",
    "        \n",
    "        # Compute entropy based on action probabilities.\n",
    "        self.entropy = -tf.reduce_sum(self.probabilities * tf.log(self.probabilities), 1, name=\"entropy\")\n",
    "        \n",
    "        # Compute losses of action probabilities associated with each observation in a single batch.\n",
    "        indices = tf.concat(values=[self.columns, self.actions], axis=1)\n",
    "        self.picked_action_prob = tf.gather_nd(self.probabilities, indices)\n",
    "        self.losses = -(tf.log(self.picked_action_prob) * self.target + self.entropy * self.e_weight)\n",
    "        \n",
    "        # Compute batch loss.\n",
    "        self.loss = tf.reduce_sum(self.losses)\n",
    "        \n",
    "        # Set optimizer.\n",
    "        self.train_op = tf.train.AdamOptimizer(self.l_rate).minimize(self.loss)\n",
    "    \n",
    "    # NOTE: computing `out` from `self.out` is not necessary -- just for debugging\n",
    "    def choose_action(self, obs, verbose=False):\n",
    "        # Compute probabilities associated with each action and output layer node values.\n",
    "        out, probs = self.sess.run([self.out, self.probabilities], feed_dict={\n",
    "            self.observations: np.array(obs).reshape(-1, 8),\n",
    "            self.training:     False\n",
    "        })\n",
    "        \n",
    "        if verbose: print(probs, out)\n",
    "            \n",
    "        # Choose action based on computed probabilities.\n",
    "        return np.random.choice(range(probs.shape[1]), p=probs.ravel())\n",
    "    \n",
    "    def train(self, act, obs, target, l_rate, e_weight):\n",
    "        length = np.array(act).reshape(-1, 1).shape[0]\n",
    "        \n",
    "        inp = (self.train_op, self.loss, self.probabilities, self.entropy, self.out)\n",
    "        \n",
    "        _, *results = self.sess.run(inp, feed_dict={\n",
    "            self.actions:      np.array(act).reshape(-1, 1),\n",
    "            self.columns:      np.arange(length).reshape(-1, 1),\n",
    "            self.e_weight:     e_weight,\n",
    "            self.l_rate:       l_rate,\n",
    "            self.observations: np.array(obs).reshape(-1, 8),\n",
    "            self.target:       np.array(target).reshape(-1, 1),\n",
    "            self.training:     True\n",
    "        })\n",
    "        \n",
    "        # print('-' * 32)\n",
    "        # print('\\n'.join(results))\n",
    "        # print('-' * 32)\n",
    "\n",
    "        return results[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(object):\n",
    "    def __init__(self, sess):\n",
    "        self.sess = sess\n",
    "        self._build()\n",
    "        \n",
    "    def _build(self):\n",
    "        self.l_rate       = tf.placeholder(tf.float32)\n",
    "        self.observations = tf.placeholder(tf.float32, (None, 8))\n",
    "        self.target       = tf.placeholder(tf.float32, (None, 1))\n",
    "        \n",
    "        with tf.variable_scope('critic-hidden'):\n",
    "            h1  = tf.layers.dense(self.observations, 128,\n",
    "                                  activation=tf.nn.relu,\n",
    "                                  kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                  name='h1')\n",
    "            \n",
    "            h2  = tf.layers.dense(h1, 64,\n",
    "                                  activation=tf.nn.relu,\n",
    "                                  kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                  name='h2')\n",
    "            \n",
    "            out = tf.layers.dense(h2, 1,\n",
    "                                  kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                  name='out')\n",
    "            \n",
    "        self.value_estimate = tf.squeeze(out) # [[num]] -> num\n",
    "        self.loss = tf.squared_difference(self.value_estimate, self.target)\n",
    "        self.train_op = tf.train.AdamOptimizer(self.l_rate).minimize(self.loss)\n",
    "        \n",
    "    def predict(self, obs):\n",
    "        return sess.run(self.value_estimate, feed_dict={\n",
    "            self.observations: np.array(obs).reshape(-1, 8)\n",
    "        })\n",
    "    \n",
    "    def update(self, obs, target, l_rate=0.01):\n",
    "        inp = (self.train_op, self.loss)\n",
    "        \n",
    "        _, *results = self.sess.run(inp, feed_dict={\n",
    "            self.l_rate:       l_rate,\n",
    "            self.observations: np.array(obs).reshape(-1, 8),\n",
    "            self.target:       np.array(target).reshape(-1, 1)\n",
    "        })\n",
    "        \n",
    "        return results[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACHandler(object):\n",
    "    def __init__(self, actor, critic, env, sess, path='./.model.ckpt'):\n",
    "        self.actor = actor\n",
    "        self.critic = critic\n",
    "        self.env = env\n",
    "        self.sess = sess\n",
    "    \n",
    "        self.saver = tf.train.Saver()\n",
    "        self.path = path\n",
    "\n",
    "    def init_vars(self):\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def run(self, train_func, rollout=100, a_rate=0.001, c_rate=0.005, decay=0.99, render=False, e_weight=0.007, **kwargs):\n",
    "        assert isinstance(train_func, str) and train_func.startswith('train_'), \\\n",
    "               'invalid train_func name specified'\n",
    "        getattr(self, train_func)(self.rollout(rollout, render, decay), a_rate, c_rate, e_weight=e_weight, **kwargs)\n",
    "        \n",
    "        # Close the display window\n",
    "        if render: self.env.close()\n",
    "            \n",
    "    def run_constant_training(self, num_episodes, a_rate=0.001, c_rate=0.005, decay=0.99, e_weight=0.007, render=False):\n",
    "        \"\"\"\n",
    "        Runs training and updates both networks during every time step\n",
    "        \"\"\"\n",
    "        for _ in range(num_episodes):\n",
    "            obs_curr = env.reset()\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "\n",
    "                if render: self.env.render()\n",
    "                action = self.actor.choose_action(obs_curr)\n",
    "\n",
    "                # Take action in environment.\n",
    "                next_obs, reward, done, _ = self.env.step(action)\n",
    "\n",
    "                next_estimate = self.critic.predict(next_obs)\n",
    "                td_target = reward + decay*next_estimate\n",
    "                td_error = td_target - self.critic.predict(obs_curr)\n",
    "                c_loss = self.critic.update(obs_curr, td_target, c_rate)\n",
    "                a_loss = self.actor.train(action, obs_curr, td_error, a_rate, e_weight)\n",
    "\n",
    "                obs_curr = next_obs\n",
    "                \n",
    "    def play(self):\n",
    "        \"\"\"\n",
    "        Runs a single instance of the game without training or storing training information\n",
    "        Always displays the game and closes the window afterward\n",
    "        \"\"\"\n",
    "        obs_curr = self.env.reset()\n",
    "        done = False\n",
    "        rewards = 0\n",
    "        while not done:\n",
    "            self.env.render()\n",
    "\n",
    "            # Agent chooses action based on difference frame.\n",
    "            action = self.actor.choose_action(obs_curr)\n",
    "\n",
    "            # Take action in environment.\n",
    "            obs_curr, reward, done, _ = self.env.step(action)\n",
    "            \n",
    "            rewards += reward\n",
    "        print('Total Reward for Episode: {}'.format(rewards))\n",
    "        env.close()\n",
    "        \n",
    "    def train_rsample(self, batch, a_rate, c_rate, e_weight=0.007, num_epochs=50, mini_batch_size=100):\n",
    "        \"\"\"\n",
    "        Performs random mini-batch training on both networks from a given\n",
    "          set of batch information\n",
    "        \"\"\"\n",
    "        for x in range(num_epochs):\n",
    "            indices = np.random.randint(len(batch['obs']), size=mini_batch_size)\n",
    "            loss = self.actor.train([batch['act'][i] for i in indices],\n",
    "                             [batch['obs'][i] for i in indices],\n",
    "                             [batch['advantage'][i] for i in indices],\n",
    "                             a_rate,\n",
    "                             e_weight)\n",
    "            self.critic.update([batch['obs'][i] for i in indices],\n",
    "                               [batch['td_target'][i] for i in indices],\n",
    "                               c_rate)\n",
    " \n",
    "    def train_all(self, batch, a_rate, c_rate, e_weight=0.007):\n",
    "        \"\"\"\n",
    "        Trains both networks on all peices of inromation in the batch\n",
    "        \"\"\"\n",
    "        self.actor.train(batch['act'],\n",
    "                         batch['obs'],\n",
    "                         batch['advantage'],\n",
    "                         a_rate,\n",
    "                         e_weight)\n",
    "        self.critic.update(batch['obs'],\n",
    "                           batch['td_target'],\n",
    "                           c_rate)\n",
    "    \n",
    "    def compute_advantage(self, obs, rewards, decay):\n",
    "        policy_target = np.zeros_like(rewards)\n",
    "        value_target = np.zeros_like(rewards)\n",
    "        running_reward = 0\n",
    "        \n",
    "\n",
    "        for idx in range(len(rewards))[:-1]:\n",
    "            next_estimate = self.critic.predict(obs[idx+1])\n",
    "            td_target = rewards[idx] + decay*next_estimate\n",
    "            td_error = td_target - self.critic.predict(obs[idx])\n",
    "            \n",
    "            policy_target[idx] = td_error\n",
    "            value_target[idx] = td_target\n",
    "        \n",
    "        return policy_target.tolist(), value_target.tolist()\n",
    "\n",
    "    def save(self):\n",
    "        self.saver.save(self.sess, self.path)\n",
    "        \n",
    "    def load(self):\n",
    "        self.saver.restore(self.sess, self.path)\n",
    "            \n",
    "    def rollout(self, count, render, decay):\n",
    "        batch = {'act': [], 'obs': [], 'rew': [], 'advantage':[], 'td_target':[]}\n",
    "        \n",
    "        for episode in range(count):\n",
    "            # Stores all the stuff\n",
    "            history = {'act': [], 'obs': [], 'rew': [], 'advantage':[], 'td_target':[]}\n",
    "            \n",
    "            obs_curr = env.reset()\n",
    "            done = False\n",
    "               \n",
    "            while not done:\n",
    "                \n",
    "                if render: self.env.render()\n",
    "                # Agent chooses action based on difference frame.\n",
    "                action = self.actor.choose_action(obs_curr, False)\n",
    "        \n",
    "                # Take action in environment.\n",
    "                next_obs, reward, done, _ = self.env.step(action)\n",
    "                \n",
    "                history['act'].append(action)\n",
    "                history['obs'].append(obs_curr)\n",
    "                history['rew'].append(reward)\n",
    "                \n",
    "                obs_curr = next_obs\n",
    "\n",
    "            # Process rewards per episode.\n",
    "            history['advantage'], history['td_target'] = self.compute_advantage(history['obs'] + obs_curr, history['rew'], decay)\n",
    "            # Add episode to batch.\n",
    "            for key in batch:\n",
    "                batch[key].extend(history[key])\n",
    "        return batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "env = gym.make('LunarLander-v2') # RGB observation space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "actor = PolicyAgent(sess)\n",
    "critic = Critic(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = ACHandler(actor, critic, env, sess, '.models/l1.cpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler.init_vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'Placeholder_6' with dtype float\n\t [[Node: Placeholder_6 = Placeholder[dtype=DT_FLOAT, shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'Placeholder_6', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/grberlstein/rl_bot/.env/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/grberlstein/rl_bot/.env/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/grberlstein/rl_bot/.env/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/grberlstein/rl_bot/.env/lib/python3.5/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 345, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 1312, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.5/asyncio/events.py\", line 125, in _run\n    self._callback(*self._args)\n  File \"/home/grberlstein/rl_bot/.env/lib/python3.5/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"/home/grberlstein/rl_bot/.env/lib/python3.5/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/grberlstein/rl_bot/.env/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/grberlstein/rl_bot/.env/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/grberlstein/rl_bot/.env/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/grberlstein/rl_bot/.env/lib/python3.5/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/grberlstein/rl_bot/.env/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/grberlstein/rl_bot/.env/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/grberlstein/rl_bot/.env/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/grberlstein/rl_bot/.env/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/grberlstein/rl_bot/.env/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/grberlstein/rl_bot/.env/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/grberlstein/rl_bot/.env/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/grberlstein/rl_bot/.env/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/grberlstein/rl_bot/.env/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-107-c297ec90c424>\", line 2, in <module>\n    actor = PolicyAgent(sess)\n  File \"<ipython-input-89-edf74993c741>\", line 10, in __init__\n    self._build()\n  File \"<ipython-input-89-edf74993c741>\", line 23, in _build\n    self.e_weight = tf.placeholder(tf.float32)\n  File \"/home/grberlstein/rl_bot/.env/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 1808, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"/home/grberlstein/rl_bot/.env/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 4848, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"/home/grberlstein/rl_bot/.env/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/grberlstein/rl_bot/.env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/grberlstein/rl_bot/.env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder_6' with dtype float\n\t [[Node: Placeholder_6 = Placeholder[dtype=DT_FLOAT, shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/rl_bot/.env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_bot/.env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_bot/.env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder_6' with dtype float\n\t [[Node: Placeholder_6 = Placeholder[dtype=DT_FLOAT, shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-bbbebc4bdfb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         handler.run('train_rsample', rollout=40, a_rate=0.001, c_rate=0.01, decay=0.97, e_weight=0.007, render=False,\n\u001b[0;32m----> 4\u001b[0;31m                    num_epochs=100, mini_batch_size=100)\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nCompleted 100 Training Iterations\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-105-dfbd7ca6cd9a>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, train_func, rollout, a_rate, c_rate, decay, render, e_weight, **kwargs)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrollout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.007\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtrain_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                \u001b[0;34m'invalid train_func name specified'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrollout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Close the display window\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-105-dfbd7ca6cd9a>\u001b[0m in \u001b[0;36mtrain_rsample\u001b[0;34m(self, batch, a_rate, c_rate, e_weight, num_epochs, mini_batch_size)\u001b[0m\n\u001b[1;32m     77\u001b[0m                              \u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'advantage'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                              \u001b[0ma_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                              e_weight)\n\u001b[0m\u001b[1;32m     80\u001b[0m             self.critic.update([batch['obs'][i] for i in indices],\n\u001b[1;32m     81\u001b[0m                                \u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'td_target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-89-edf74993c741>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, act, obs, target, l_rate, e_weight)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml_rate\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0ml_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m:\u001b[0m      \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         })\n\u001b[1;32m     92\u001b[0m \u001b[0;31m#         print('--------------')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_bot/.env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_bot/.env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_bot/.env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_bot/.env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder_6' with dtype float\n\t [[Node: Placeholder_6 = Placeholder[dtype=DT_FLOAT, shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'Placeholder_6', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/grberlstein/rl_bot/.env/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/grberlstein/rl_bot/.env/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/grberlstein/rl_bot/.env/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/grberlstein/rl_bot/.env/lib/python3.5/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 345, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 1312, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.5/asyncio/events.py\", line 125, in _run\n    self._callback(*self._args)\n  File \"/home/grberlstein/rl_bot/.env/lib/python3.5/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"/home/grberlstein/rl_bot/.env/lib/python3.5/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/grberlstein/rl_bot/.env/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/grberlstein/rl_bot/.env/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/grberlstein/rl_bot/.env/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/grberlstein/rl_bot/.env/lib/python3.5/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/grberlstein/rl_bot/.env/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/grberlstein/rl_bot/.env/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/grberlstein/rl_bot/.env/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/grberlstein/rl_bot/.env/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/grberlstein/rl_bot/.env/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/grberlstein/rl_bot/.env/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/grberlstein/rl_bot/.env/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/grberlstein/rl_bot/.env/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/grberlstein/rl_bot/.env/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-107-c297ec90c424>\", line 2, in <module>\n    actor = PolicyAgent(sess)\n  File \"<ipython-input-89-edf74993c741>\", line 10, in __init__\n    self._build()\n  File \"<ipython-input-89-edf74993c741>\", line 23, in _build\n    self.e_weight = tf.placeholder(tf.float32)\n  File \"/home/grberlstein/rl_bot/.env/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 1808, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"/home/grberlstein/rl_bot/.env/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 4848, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"/home/grberlstein/rl_bot/.env/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/grberlstein/rl_bot/.env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/grberlstein/rl_bot/.env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder_6' with dtype float\n\t [[Node: Placeholder_6 = Placeholder[dtype=DT_FLOAT, shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "    for _ in range(100):\n",
    "        handler.run('train_rsample', rollout=40, a_rate=0.001, c_rate=0.01, decay=0.97, e_weight=0.007, render=False,\n",
    "                   num_epochs=100, mini_batch_size=100)\n",
    "        print('-',end='')\n",
    "    print('\\nCompleted 100 Training Iterations\\n')\n",
    "    handler.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(True):\n",
    "    for _ in range(100):\n",
    "        handler.run('train_all', rollout=30, a_rate=0.001, c_rate=0.005, decay=0.99, render=False)\n",
    "        print('-',end='')\n",
    "    print('Completed 100 Training Iterations\\n')\n",
    "    handler.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler.run_constant_training(10000,render=True, decay=0.99, a_rate=0.002, c_rate=0.01, e_weight=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from .models/l1.cpt\n"
     ]
    }
   ],
   "source": [
    "handler.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward for Episode: -46.34610782563561\n",
      "Total Reward for Episode: -215.93202158955063\n",
      "Total Reward for Episode: -61.82729807257837\n",
      "Total Reward for Episode: 107.70510878849339\n",
      "Total Reward for Episode: -272.9352045647684\n",
      "Total Reward for Episode: 84.7154958241119\n",
      "Total Reward for Episode: -94.80646271212674\n",
      "Total Reward for Episode: -104.57005108570428\n",
      "Total Reward for Episode: -68.05701209406867\n",
      "Total Reward for Episode: 126.80516327854224\n",
      "Total Reward for Episode: -113.74487616521319\n",
      "Total Reward for Episode: -245.89870312583298\n",
      "Total Reward for Episode: 52.90281767657473\n",
      "Total Reward for Episode: -73.00809814949372\n",
      "Total Reward for Episode: -92.42599453655066\n",
      "Total Reward for Episode: -121.58708441703564\n",
      "Total Reward for Episode: -134.75952039104175\n",
      "Total Reward for Episode: -59.72341063114468\n",
      "Total Reward for Episode: -102.5078249677441\n",
      "Total Reward for Episode: -82.0380089023412\n",
      "Total Reward for Episode: -93.89252544934656\n",
      "Total Reward for Episode: -81.26268950021465\n",
      "Total Reward for Episode: -84.46424920447411\n",
      "Total Reward for Episode: -80.33818110742469\n",
      "Total Reward for Episode: -88.47356812209269\n",
      "Total Reward for Episode: -114.40634683726665\n",
      "Total Reward for Episode: -51.995929227716665\n",
      "Total Reward for Episode: -107.26311765321238\n",
      "Total Reward for Episode: 72.14975474941471\n",
      "Total Reward for Episode: 86.01753164587653\n",
      "Total Reward for Episode: -59.47801845804977\n",
      "Total Reward for Episode: -69.2786931388378\n",
      "Total Reward for Episode: -51.27528658654384\n",
      "Total Reward for Episode: -95.33294266295184\n",
      "Total Reward for Episode: -81.84053760022114\n",
      "Total Reward for Episode: 93.89096692215043\n",
      "Total Reward for Episode: -70.5521899592423\n",
      "Total Reward for Episode: -99.25017010692544\n",
      "Total Reward for Episode: -96.71748430377744\n",
      "Total Reward for Episode: -248.04108233036027\n",
      "Total Reward for Episode: -37.05715217045572\n",
      "Total Reward for Episode: -62.538824655454505\n",
      "Total Reward for Episode: -65.40384043375958\n",
      "Total Reward for Episode: -67.76097611964947\n",
      "Total Reward for Episode: -49.167445555479816\n",
      "Total Reward for Episode: -27.24920050416104\n",
      "Total Reward for Episode: -47.60026145659438\n",
      "Total Reward for Episode: -297.30193374232414\n",
      "Total Reward for Episode: 65.51476906126302\n",
      "Total Reward for Episode: -233.11544189845316\n",
      "Total Reward for Episode: -88.86260400944663\n",
      "Total Reward for Episode: -86.46455547379854\n",
      "Total Reward for Episode: -228.12604216275494\n",
      "Total Reward for Episode: -44.971432090437304\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-ca9dcd83240f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-79-cc89008dc29a>\u001b[0m in \u001b[0;36mplay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;31m# Agent chooses action based on difference frame.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_bot/.env/lib/python3.5/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_bot/.env/lib/python3.5/site-packages/gym/envs/box2d/lunar_lander.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_polygon\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflagy2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflagy2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mSCALE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mSCALE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflagy2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mSCALE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_bot/.env/lib/python3.5/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, return_rgb_array)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mgeom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgeom\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monetime_geoms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0mgeom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_bot/.env/lib/python3.5/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0mattr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_bot/.env/lib/python3.5/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36menable\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mglPushMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0mglTranslatef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# translate to GL loc ppint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mglRotatef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRAD2DEG\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrotation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0mglScalef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_bot/.env/lib/python3.5/site-packages/pyglet/gl/lib.py\u001b[0m in \u001b[0;36merrcheck\u001b[0;34m(result, func, arguments)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mGLException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No GL context; create a Window first'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gl_begin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglGetError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgluErrorString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while 1: handler.play()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
