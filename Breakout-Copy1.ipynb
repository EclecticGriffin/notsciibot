{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================================#\n",
    "# Yes, this notebook is over-commented. #\n",
    "#=======================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make notebook span entire screen, horizontally.\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "#  -> fix loss funtion\n",
    "#  -> check reward processing\n",
    "\n",
    "class PolicyAgent(object):\n",
    "    def __init__(self):\n",
    "        self.n_actions = 4\n",
    "        \n",
    "        # Build graph (i.e. internal neural network).\n",
    "        self._build()\n",
    "        \n",
    "        # Create environment within which the graph will train.\n",
    "        self.sess = tf.Session()\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "        # Evaluate an tf.Operation (returned by global_variables_initializer()) that initializes TRAINABLE VARIABLES.\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def _build(self):\n",
    "        self.obs      = tf.placeholder(tf.float32, (None, 210, 160, 3))\n",
    "        self.acts     = tf.placeholder(tf.int32, (None,))\n",
    "        self.rew      = tf.placeholder(tf.float32, (None,))\n",
    "        self.l_rate   = tf.placeholder(tf.float32)\n",
    "        self.training = tf.placeholder(tf.bool)\n",
    "        \n",
    "        c1 = tf.layers.conv2d(self.obs, 16, (3,3), name='c1')\n",
    "        m1 = tf.layers.max_pooling2d(c1,[2,2],[2,2], name='m1')\n",
    "        c2 = tf.layers.conv2d(m1, 32, [3,3], name='c2')\n",
    "        m2 = tf.layers.max_pooling2d(c2,[2,2],[1,1], name='m2')\n",
    "        flat = tf.layers.flatten(m2, name='flat')\n",
    "        \n",
    "        h1 = tf.layers.dense(flat, 64, activation=tf.nn.relu, kernel_initializer=tf.random_normal_initializer(), name='h1')\n",
    "        h2 = tf.layers.dense(h1,   64, activation=tf.nn.relu, kernel_initializer=tf.random_normal_initializer(), name='h2')\n",
    "        \n",
    "        dropout = tf.layers.dropout(h2, training=self.training, name='dropout')\n",
    "        \n",
    "        # This needs to have linear activations to conform to the \"unscaled log probabilities\" requirement of `sparse_softmax_cross_entropy_loss_with_logits()`.\n",
    "        out = tf.layers.dense(dropout, self.n_actions, kernel_initializer=tf.random_normal_initializer(), name='out')\n",
    "        \n",
    "        # Compute normalized probabilities associated with each action.\n",
    "        self.probabilities = tf.nn.softmax(out)\n",
    "        \n",
    "        # Returns softmax cross entropy loss.\n",
    "        neg_log_prob = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=out, labels=self.acts)\n",
    "        loss = tf.reduce_mean(neg_log_prob * self.rew)\n",
    "        \n",
    "        # Set optimizer.\n",
    "        self.train_op = tf.train.AdamOptimizer(self.l_rate).minimize(loss)\n",
    "    \n",
    "    def save(self, path):\n",
    "        self.saver.save(self.sess, path)\n",
    "        \n",
    "    def load(self, path):\n",
    "        self.saver.restore(self.sess, path)\n",
    "    \n",
    "    def choose_action(self, obs):\n",
    "        # Compute probabilities associated with each action.\n",
    "        prob_weights = self.sess.run(self.probabilities, feed_dict={\n",
    "            self.obs:      np.array(obs).reshape(-1, 210, 160, 3),\n",
    "            self.training: False\n",
    "        })\n",
    "        \n",
    "        # Choose action based on computed probabilities.\n",
    "        return np.random.choice(range(prob_weights.shape[1]), p=prob_weights.ravel())\n",
    "    \n",
    "    def train(self, act, obs, rew, l_rate):\n",
    "        self.sess.run(self.train_op, feed_dict={\n",
    "            self.obs:      np.array(obs).reshape(-1, 210, 160, 3),\n",
    "            self.acts:     act,\n",
    "            self.rew:      rew,\n",
    "            self.l_rate:   l_rate,\n",
    "            self.training: True\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentHandler(object):\n",
    "    def __init__(self, agent, env, path='./.model.ckpt'):\n",
    "        self.agent = agent\n",
    "        self.env = env\n",
    "        \n",
    "        self.saver = tf.train.Saver() # saves all variables\n",
    "        self.path = path\n",
    "        \n",
    "    def run(self, train_func, rollout=100, l_rate=0.001, **kwargs):\n",
    "        assert isinstance(train_func, str) and train_func.startswith('train_'), \\\n",
    "               'invalid train_func name specified'\n",
    "        getattr(self, train_func)(self.rollout(rollout), l_rate, **kwargs)\n",
    "        \n",
    "    def train_rsample(self, batch, l_rate, num_epochs=30, mini_batch_size=50):\n",
    "        indices = np.random.randint(len(batch['rew']), size=mini_batch_size)\n",
    "        self.agent.train([batch['act'][i] for i in indices],\n",
    "                         [batch['obs'][i] for i in indices],\n",
    "                         [batch['rew'][i] for i in indices],\n",
    "                         l_rate)\n",
    "        \n",
    "    def process_rewards(self, rewards, decay=0.99):\n",
    "        discounted = np.zeros_like(rewards)\n",
    "        running_reward = 0\n",
    "        \n",
    "        for idx in reversed(range(len(rewards))):\n",
    "            running_reward += rewards[idx]\n",
    "            running_reward *= decay\n",
    "            discounted[idx] = running_reward\n",
    "            \n",
    "        discounted -= np.mean(discounted)\n",
    "        if np.std(discounted) != 0:\n",
    "            discounted /= np.std(discounted)\n",
    "        return discounted.tolist()\n",
    "\n",
    "    def save(self):\n",
    "        self.agent.save(self.path)\n",
    "        \n",
    "    def load(self):\n",
    "        self.agent.restore(self.path)\n",
    "            \n",
    "    def rollout(self, count, render=True):\n",
    "        batch = {'act': [], 'obs': [], 'rew': []}\n",
    "        \n",
    "        for episode in range(count):\n",
    "            # Stores all the stuff\n",
    "            history = {'act': [], 'obs': [], 'rew': []}\n",
    "            \n",
    "            # Get observation of initial state of environment.\n",
    "            obs_prev = self.env.reset()\n",
    "            \n",
    "            # Randomly choose first action in order to create first difference frame.\n",
    "            obs_curr, reward, done, _ = self.env.step(env.action_space.sample())\n",
    "\n",
    "            while not done:\n",
    "                # Make difference frame.\n",
    "                diff_frame, obs_prev = obs_curr - obs_prev, obs_curr\n",
    "        \n",
    "                if render: self.env.render()\n",
    "        \n",
    "                # Agent chooses action based on difference frame.\n",
    "                action = self.agent.choose_action(diff_frame)\n",
    "        \n",
    "                # Take action in environment.\n",
    "                curr_obs, reward, done, _ = self.env.step(action)\n",
    "                \n",
    "                history['act'].append(action)\n",
    "                history['obs'].append(curr_obs)\n",
    "                history['rew'].append(reward)\n",
    "                \n",
    "            # Process rewards per episode.\n",
    "            history['rew'] = self.process_rewards(history['rew'])\n",
    "            \n",
    "            # Add episode to batch.\n",
    "            for key in batch:\n",
    "                batch[key].extend(history[key])\n",
    "                \n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Breakout-v0') # RGB observation space\n",
    "agent = PolicyAgent()\n",
    "\n",
    "handler = AgentHandler(agent, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler.run('train_rsample', rollout=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
