{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================================#\n",
    "# Yes, this notebook is over-commented. #\n",
    "#=======================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make notebook span entire screen, horizontally.\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Graveyard Snippits:\n",
    "#--------------\n",
    "# indicies = tf.concat(values=[self.col, self.acts], axis=1)\n",
    "# self.picked_action_prob = tf.gather_nd(self.probabilities,indicies)\n",
    "# self.losses = -tf.log(tf.clip_by_value(self.picked_action_prob,1e-10,1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "#  -> fix loss funtion\n",
    "#  -> check reward processing\n",
    "\n",
    "class PolicyAgent(object):\n",
    "    def __init__(self, sess):\n",
    "        self.n_actions = 4\n",
    "        \n",
    "        # Build graph (i.e. internal neural network).\n",
    "        self._build()\n",
    "        \n",
    "        # Create environment within which the graph will train.\n",
    "        self.sess = sess\n",
    "        \n",
    "        # Evaluate an tf.Operation (returned by global_variables_initializer()) that initializes TRAINABLE VARIABLES.\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def _build(self):\n",
    "        self.obs      = tf.placeholder(tf.float32, (None, 8))\n",
    "        self.acts     = tf.placeholder(tf.int32, (None,1))\n",
    "        self.target   = tf.placeholder(tf.float32, (None,1))\n",
    "        self.l_rate   = tf.placeholder(tf.float32)\n",
    "        self.training = tf.placeholder(tf.bool)\n",
    "\n",
    "        with tf.variable_scope('actor-hidden'):\n",
    "            h1 = tf.layers.dense(self.obs, 128, \n",
    "                                 activation=tf.nn.relu, \n",
    "                                 kernel_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                 name='h1')\n",
    "            drop1 = tf.layers.dropout(h1, training=self.training, \n",
    "                                      name='drop1')\n",
    "            h2 = tf.layers.dense(drop1,   64, activation=tf.nn.relu, \n",
    "                                 kernel_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                 name='h2')\n",
    "            drop2 = tf.layers.dropout(h2, training=self.training, \n",
    "                                      name='drop2')\n",
    "            h3 = tf.layers.dense(drop2,   32, activation=tf.nn.relu, \n",
    "                                 kernel_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                 name='h3')\n",
    "            drop3 = tf.layers.dropout(h3, training=self.training, name='dropout')\n",
    "        \n",
    "            # This needs to have linear activations to conform to the \"unscaled log probabilities\" \n",
    "            #requirement of `sparse_softmax_cross_entropy_loss_with_logits()`.\n",
    "            self.out = tf.layers.dense(drop3, self.n_actions, \n",
    "                                       kernel_initializer=tf.random_normal_initializer(), \n",
    "                                       name='out')\n",
    "            # Clip values to avoid nans\n",
    "            clipped_out = tf.clip_by_value(self.out,20,10000) # Trial and error produced the lower bound,\n",
    "            \n",
    "        \n",
    "        # Compute probabilities associated with each action.\n",
    "        self.probabilities = tf.nn.softmax(clipped_out)\n",
    "        \n",
    "        # Compute entropy based on probabilities\n",
    "        self.entropy = -tf.reduce_sum(self.probabilities * tf.log(tf.clip_by_value(self.probabilities,1e-10,1.0)), \n",
    "                                      1, name=\"entropy\")\n",
    "        # Compute losses\n",
    "        self.neg_log = tf.losses.sparse_softmax_cross_entropy(labels=self.acts, logits=clipped_out)\n",
    "        self.loss = tf.reduce_sum((self.neg_log * self.target) - self.entropy * 0.005)\n",
    "        \n",
    "        # Set optimizer.\n",
    "        self.train_op = tf.train.AdamOptimizer(self.l_rate).minimize(self.loss)\n",
    "    \n",
    "    def choose_action(self, obs, p_flag=False):\n",
    "        # Compute probabilities associated with each action.\n",
    "        out_vals, prob_weights = self.sess.run([self.out, self.probabilities], feed_dict={\n",
    "            self.obs:      np.array(obs).reshape(-1, 8),\n",
    "            self.training: False\n",
    "        })\n",
    "        \n",
    "        if p_flag: print(prob_weights, out_vals)\n",
    "        # Choose action based on computed probabilities.\n",
    "        return np.random.choice(range(prob_weights.shape[1]), p=prob_weights.ravel())\n",
    "    \n",
    "    def train(self, act, obs, target, l_rate):\n",
    "#         print(np.array(act).reshape(-1,1))\n",
    "        _, loss, entropy, prob, neg_log, out = self.sess.run([self.train_op,self.loss, self.entropy, \n",
    "                                                              self.probabilities, self.neg_log, self.out], feed_dict={\n",
    "            self.obs:      np.array(obs).reshape(-1, 8),\n",
    "            self.acts:     np.array(act).reshape(-1,1),\n",
    "            self.target:   np.array(target).reshape(-1,1),\n",
    "            self.l_rate:   l_rate,\n",
    "            self.training: True\n",
    "        })\n",
    "#         print(prob)\n",
    "#         print(entropy)\n",
    "#         print(neg_log)\n",
    "#         print(out)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(object):\n",
    "    def __init__(self, sess):\n",
    "        self.sess = sess\n",
    "        self._build()\n",
    "        \n",
    "    def _build(self):\n",
    "        self.obs     = tf.placeholder(tf.float32, (None, 8))   # The input state\n",
    "        self.target  = tf.placeholder(tf.float32, (None, 1))             # The target output score\n",
    "        self.lr      = tf.placeholder(tf.float32)                        # Learning Rate\n",
    "\n",
    "        \n",
    "        with tf.variable_scope('critic-hidden'):\n",
    "            h1 = tf.layers.dense(self.obs, 128, activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                 name='h1')\n",
    "            h2 = tf.layers.dense(h1, 64, activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                 name='h2')\n",
    "            out = tf.layers.dense(h2, 1, kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                 name='out')\n",
    "            \n",
    "        self.value_estimate = tf.squeeze(out)   # [[num]] -> num\n",
    "        self.loss = tf.squared_difference(self.value_estimate, self.target)\n",
    "        \n",
    "        self.train_op = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(self.loss)\n",
    "        \n",
    "    def predict(self, obs):\n",
    "        return sess.run(self.value_estimate,feed_dict={self.obs: np.array(obs).reshape(-1, 8)})\n",
    "    \n",
    "    def update(self, obs, target, learning_rate=0.01):\n",
    "        _, loss = self.sess.run([self.train_op, self.loss], feed_dict={self.obs:     np.array(obs).reshape(-1, 8),\n",
    "                                                                       self.target:  np.array(target).reshape(-1,1),\n",
    "                                                                       self.lr:      learning_rate})\n",
    "        return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACHandler(object):\n",
    "    def __init__(self, actor, critic, sess, env, path='./.model.ckpt'):\n",
    "        self.actor = actor\n",
    "        self.critic = critic\n",
    "        self.env = env\n",
    "        self.sess = sess\n",
    "    \n",
    "        self.saver = tf.train.Saver() # saves all variables\n",
    "        self.path = path\n",
    "        \n",
    "        \n",
    "        \n",
    "    def init_vars(self):\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def run(self, train_func, rollout=100, a_rate=0.001, c_rate=0.005, decay=0.99, render=False, **kwargs):\n",
    "        assert isinstance(train_func, str) and train_func.startswith('train_'), \\\n",
    "               'invalid train_func name specified'\n",
    "        getattr(self, train_func)(self.rollout(rollout, render, decay), a_rate, c_rate, **kwargs)\n",
    "        \n",
    "        # Close the display window\n",
    "        if render: self.env.close()\n",
    "            \n",
    "    def run_constant_training(self, num_episodes, a_rate=0.001, c_rate=0.005, decay=0.99, render=False):\n",
    "        \"\"\"\n",
    "        Runs training and updates both networks during every time step\n",
    "        \"\"\"\n",
    "        for _ in range(num_episodes):\n",
    "            obs_curr = env.reset()\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "\n",
    "                if render: self.env.render()\n",
    "                action = self.actor.choose_action(obs_curr)\n",
    "\n",
    "                # Take action in environment.\n",
    "                next_obs, reward, done, _ = self.env.step(action)\n",
    "\n",
    "                next_estimate = self.critic.predict(next_obs)\n",
    "                td_target = reward + decay*next_estimate\n",
    "                td_error = td_target - self.critic.predict(obs_curr)\n",
    "                c_loss = self.critic.update(obs_curr, td_target, c_rate)\n",
    "                a_loss = self.actor.train(action, obs_curr, td_error, a_rate)\n",
    "\n",
    "                obs_curr = next_obs\n",
    "                \n",
    "    def play(self):\n",
    "        \"\"\"\n",
    "        Runs a single instance of the game without training or storing training information\n",
    "        Always displays the game and closes the window afterward\n",
    "        \"\"\"\n",
    "        obs_curr = self.env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            self.env.render()\n",
    "\n",
    "            # Agent chooses action based on difference frame.\n",
    "            action = self.actor.choose_action(obs_curr)\n",
    "\n",
    "            # Take action in environment.\n",
    "            next_obs, reward, done, _ = self.env.step(action)\n",
    "        env.close()\n",
    "        \n",
    "    def train_rsample(self, batch, a_rate, c_rate, num_epochs=50, mini_batch_size=100):\n",
    "        \"\"\"\n",
    "        Performs random mini-batch training on both networks from a given\n",
    "          set of batch information\n",
    "        \"\"\"\n",
    "        for _ in range(num_epochs):\n",
    "            indices = np.random.randint(len(batch['obs']), size=mini_batch_size)\n",
    "            self.actor.train([batch['act'][i] for i in indices],\n",
    "                             [batch['obs'][i] for i in indices],\n",
    "                             [batch['advantage'][i] for i in indices],\n",
    "                             a_rate)\n",
    "            self.critic.update([batch['obs'][i] for i in indices],\n",
    "                               [batch['td_target'][i] for i in indices],\n",
    "                               c_rate)\n",
    "        \n",
    "    def train_all(self, batch, a_rate, c_rate):\n",
    "        \"\"\"\n",
    "        Trains both networks on all peices of inromation in the batch\n",
    "        \"\"\"\n",
    "        self.actor.train(batch['act'],\n",
    "                         batch['obs'],\n",
    "                         batch['advantage'],\n",
    "                         a_rate)\n",
    "        self.critic.update(batch['obs'],\n",
    "                           batch['td_target'],\n",
    "                           c_rate)\n",
    "    \n",
    "    def compute_advantage(self, obs, rewards, decay):\n",
    "        policy_target = np.zeros_like(rewards)\n",
    "        value_target = np.zeros_like(rewards)\n",
    "        running_reward = 0\n",
    "        \n",
    "        for idx in reversed(range(len(rewards))):\n",
    "            running_reward += rewards[idx] + decay*running_reward\n",
    "            policy_target[idx] = running_reward - self.critic.predict(obs[idx])\n",
    "            value_target[idx] = running_reward\n",
    "        \n",
    "        return policy_target.tolist(), value_target.tolist()\n",
    "\n",
    "    def save(self):\n",
    "        self.saver.save(self.sess, self.path)\n",
    "        \n",
    "    def load(self):\n",
    "        self.saver.restore(self.sess, self.path)\n",
    "            \n",
    "    def rollout(self, count, render, decay):\n",
    "        batch = {'act': [], 'obs': [], 'rew': [], 'advantage':[], 'td_target':[]}\n",
    "        \n",
    "        for episode in range(count):\n",
    "            # Stores all the stuff\n",
    "            history = {'act': [], 'obs': [], 'rew': [], 'advantage':[], 'td_target':[]}\n",
    "            \n",
    "            obs_curr = env.reset()\n",
    "            done = False\n",
    "               \n",
    "            while not done:\n",
    "                \n",
    "                if render: self.env.render()\n",
    "                # Agent chooses action based on difference frame.\n",
    "                action = self.actor.choose_action(obs_curr, False)\n",
    "        \n",
    "                # Take action in environment.\n",
    "                next_obs, reward, done, _ = self.env.step(action)\n",
    "                \n",
    "                history['act'].append(action)\n",
    "                history['obs'].append(obs_curr)\n",
    "                history['rew'].append(reward)\n",
    "                \n",
    "                obs_curr = next_obs\n",
    "\n",
    "            # Process rewards per episode.\n",
    "            history['advantage'], history['td_target'] = self.compute_advantage(history['obs'], history['rew'], decay)\n",
    "\n",
    "            # Add episode to batch.\n",
    "            for key in batch:\n",
    "                batch[key].extend(history[key])\n",
    "#             print(sum(history['rew'])) \n",
    "        return batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "env = gym.make('LunarLander-v2') # RGB observation space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "actor = PolicyAgent(sess)\n",
    "critic = Critic(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = ACHandler(actor, critic, sess, env, '.models/l1.cpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler.init_vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(True):\n",
    "    for _ in range(100):\n",
    "        handler.run('train_rsample', rollout=30, a_rate=0.001, c_rate=0.005, decay=0.99, render=True,\n",
    "                   num_epochs=50, mini_batch_size=100)\n",
    "        print('-',end='')\n",
    "    print('Completed 100 Training Iterations\\n')\n",
    "    handler.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(True):\n",
    "    for _ in range(100):\n",
    "        handler.run('train_all', rollout=30, a_rate=0.001, c_rate=0.005, decay=0.99, render=True)\n",
    "        print('-',end='')\n",
    "    print('Completed 100 Training Iterations\\n')\n",
    "    handler.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler.run_constant_training(100,render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "handler.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler.play()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
